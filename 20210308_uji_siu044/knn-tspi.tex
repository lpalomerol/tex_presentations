\documentclass[12pt, aspectratio=169]{beamer} % aspectratio = either 43 or 169
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage[T1]{fontenc}
\graphicspath{ {./img/} }

\usetheme{Madrid}
\mode<presentation>

% title and author
\title{A study of the use of complexity measures in the similarity search process adopted by kNN algorithm for time series predictionn}
\author{Parmezan, Antonio Rafael Sabino, Batista, Gustavo E.A.P.A.}

% document
\begin{document}


\frame{\titlepage}


\begin{frame}{Overview}
\tableofcontents
\end{frame}

\section{Introducción}

\begin{frame}{El problema de la predicción}
  \begin{itemize}
  \item La predicción del futuro siempre ha sido un problema.
  \item Toda serie temporal contiene un conjunto de patrones ocultos e interacciones difíciles de identificar e interpretar.
  \item Los algoritmos actuales:
    \begin{enumerate}
    \item Asumen e identifican características de los datos de la serie temporal.
    \item Generan un modelo explicativo para los datos actuales.
    \item Mediante extrapolación permiten hacer predicciones.
    \end{enumerate}
  \item Existen dos grandes filosofías de aproximación: \textbf{paramétricas} y \textbf{no paramétricas}
  \end{itemize}
\end{frame}

\begin{frame}{Aproximaciones paramétricas}

  \begin{block}{Aproximaciones paramétricas}

    Basados principalmente en modelos de autorregresión y medias móviles

    \begin{itemize}
      \item Medias Móviles
      \item Modelos de Holt - Holt Winters
      \item Modelos ARIMA - SARIMA
        \[ y'_t = c + \varphi_1y'_{t-1} + \ldots + \varphi_py'_{t-p} +
          \theta_1\varepsilon_{t-1} + \ldots + \theta_q\varepsilon_{t-q} + \varepsilon_t\]
    \end{itemize}
  \end{block}
\end{frame}

\begin{frame}{Aproximaciones no paramétricas}
    Son aproximaciones basadas en técnicas de Machine Learning.
    
    Existen dos grandes enfoques: globales y locales
    \begin{block}{Enfoque global}
      \begin{itemize}
      \item Se entrena el modelo utilizando toda la serie temporal.
      \item Ejemplos: Redes neuronales y Support Vector Machine.
      \end{itemize}
    \end{block}

    \begin{block}{Enfoque local}
      \begin{itemize}
      \item Se buscan similitudes entre secciones de la serie temporal.
      \item Ejemplos: Técnicas Nearest Neighbours (kNN)
      \end{itemize}
    \end{block}

\end{frame}



\section{kNN - TSPI}
\begin{frame}{kNN en series temporales}
  Esta aproximación busca similitudes en todo el histórico de la serie y realiza predicciones en base a la serie o series más similares.
  La predicción busca las \textit{k} series más similares de \textit{l} elementos. En este caso las \textit{k} = 3 y \textbf{l} = 25.
  \begin{figure}
  \includegraphics[width=8cm]{20210312_1_knn_summary.png}
  \label{fig:ejemplo_knn_ts}
\end{figure}
\end{frame}

\begin{frame}{kNN en series temporales}
  Esta aproximación busca similitudes en todo el histórico de la serie y realiza predicciones en base a la serie o series más similares.
  La predicción busca las \textit{k} series más similares de \textit{l} elementos. En este caso las \textit{k} = 3 y \textbf{l} = 25.
  \begin{figure}
  \includegraphics[width=8cm]{20210312_1_knn_summary.png}
  \label{fig:ejemplo_knn_ts}
\end{figure}
\textbf{La aproximación básica tiene tres grandes problemas.}
\end{frame}

\begin{frame}{Los tres grandes problemas de kNN}

  \begin{itemize}
  \item Problema 1: Amplitud y desfase.
  \item Problema 2: Gestión de la varianza en los objetos complejos.
  \item Problema 3: Tratamiento de las asociaciones triviales.
  \end{itemize}

  \begin{figure}
    \includegraphics[width=1\textwidth]{20210313_1_tres_metricas.png}
  \end{figure}
  
\end{frame}

\begin{frame}{Los tres grandes problemas}{Problema 1: Amplitud y desfase}
  La secuencia S\textsuperscript{1} es igual a la secuencia a testear (Q) más un pequeño desfase.
  El error acumulado por la diferencia de amplitudes y desfases provoca penalizaciones importantes.

  \begin{figure}
      \includegraphics[width=8cm]{20210312_2_problema_offset.png}
      \label{fig:problema_1}
  \end{figure}
  En este caso se considera S\textsuperscript{2} más similar a Q que S\textsuperscript{1}
\end{frame}

\begin{frame}{Los tres grandes problemas}{Problema 2:  Gestión de la varianza en los objetos complejos}
  La acumulación de errores menores debido a series con alta variabilidad intrínseca (objetos complejos) penaliza mucho contra los objetos simples.

  \begin{figure}
  \includegraphics[width=8cm]{20210312_3_problema_complejos.png}
  \label{fig:problema_2}
\end{figure}

  En este caso la secuencia simple S\textsuperscript{2} se considera  más similar a Q que S\textsuperscript{1}

\end{frame}

\begin{frame}{Los tres grandes problemas}{Problema 3: Tratamiento de las asociaciones triviales}
  En este caso las tres secuencias comparten muchas observaciones.
  Este caso ocurre cuando se incluye la secuencia a buscar en \textit{pool} de secuencias elegibles, o bien hay aciertos fortuitos.
  \begin{figure}
    \includegraphics[width=8cm]{20210312_4_problema_triviales.png}
    \label{fig:problema_3}
  \end{figure}
  
\end{frame}

\begin{frame}{Los tres grandes problemas}{Soluciones}
  \begin{block}{Problema 1: Amplitud y desfase}
    Solución: Transformar las series para que sean invariantes en amplitud y desfase. Por ejemplo aplicando la \textit{z-normalización}.
    
  \end{block}
  \begin{block}{Problema 2: Gestión de la varianza en los objetos complejos}
    Solución: Usar una distancia invariante a la complejidad. 
  \end{block}
  \begin{block}{Problema 3: Tratamiento de las asociaciones triviales}
    Solución: Excluir automáticamente las asociaciones triviales, en este caso utilizando un chequeo iterativo.
  \end{block}
\end{frame}



\begin{frame}{kNN-TSPI}
  \begin{columns}
    \begin{column}{0.5\textwidth}
      El algoritmo kNN-Time Series Prediction with Invariances (kNN-TSPI) es una variante del kNN que tiene en cuenta las soluciones de los tres problemas anteriores:
      \begin{itemize}
      \item Problema 1: Z-normalización en la línea 3
      \item Problema 2: Aplicación de la distancia invariante en la línea 5*
      \item Problema 3: Identificación de los \textit{k} vecinos más cercanos en la línea 6
      \end{itemize}
      
    \end{column}
    \begin{column}{0.5\textwidth}
        \begin{figure}
          \includegraphics[width=0.8\textwidth]{20210313_2_algoritmo.png}
          \label{fig:algoritmo}
        \end{figure}

    \end{column}                
  \end{columns}

  
\end{frame}

\begin{frame}{Notas sobre la CID}{Definición de la CID}
  La distancia de complejidad invariante usa iformación sobre la distancia euclideana (ED) ajustada por un factor de complejidad (CF).
  Este factor de complejidad se calcula, en este caso, a partir de una estimación de la complejidad cuadrática (CE). 

  \[ CID(Q,C) = ED(Q,C) \times CF(Q,C) \]

  \[ CF(Q,C) = \frac
    { \max{ (CE(Q),CE(C))} }
    { \min{(CE(Q),CE(C))}  }
  \]

  \[ CE(Q) = \sqrt{ \sum_{i=1}^{n=1}{  ( q_i - q_{i+1} )^2 } }  \]

Existen muchas variantes de la \textit{CE}: Diferencia absoluta, Compresión, Vértices, Zero-crossing, Permutación de entropía, etc. ¿Cuál es mejor?


\end{frame}

\begin{frame}{Notas sobre la CID}{Evaluación empírica}
  Se evalúan las seis medidas presentadas junto con la distancia euclideana no ajustada.
  \begin{columns}
    \begin{column}{0.5\textwidth}
      \begin{itemize}
        \item Paso 1: Selección de 55 series del repositorio ICMC-USP TSPR.
        \item Paso 2: Para cada aproximación en cada serie, se estiman los parámetros usando el método de Mean Square Error y se realizan las predicciones.
        \item Paso 3: Para cada aproximación en cada serie, se realizan las predicciones y se comparan usando tres tipos de distancias diferentes: MAPE, Coeficiente Theil U y POCID
      \end{itemize}
    \end{column}

    \begin{column}{0.5\textwidth}
      \begin{figure}
        \includegraphics[width=\textwidth]{20210313_3_evaluacion_cid.png}
      \end{figure}
    \end{column}

    \end{columns}
  
\end{frame}


\section{Resultados}

\begin{frame}{Evaluación empírica de las distancias}{MAPE}
  \begin{columns}
    \begin{column}{0.6\textwidth}
      \begin{block}{MAPE}
        \begin{itemize}
        \item No se observan diferencias significativas entre las medidas.
        \item Los dos mejores son la diferencia cuadrática y Zero-crossing.
          \end{itemize}
      \end{block}
    \end{column}
    \begin{column}{0.4\textwidth}
      \begin{figure}
        \includegraphics[width=\textwidth]{20210313_4_nemenyi_distancias_cd.png}
      \end{figure}
    \end{column}
  \end{columns}
\end{frame}

\begin{frame}{Evaluación empírica de las distancias}{POCID / Theil U}

  \begin{columns}
    \begin{column}{0.6\textwidth}
      \begin{block}{POCID}
        \begin{itemize}
        \item Los promedios y SD parecen distribuidos uniformemente en el rango 57-60\%.
        \item El mejor resultado corresponde a la diferencia cuadrática.
        \end{itemize}
      \end{block}
      \begin{block}{Theil's U}
        \begin{itemize}
        \item Permutation Entropy muestra el mejor resultado en TU < 1.
          \item Diferencia absoluta muestra el mejor resultado en TU $\leq$ 0.55.
        \end{itemize}
      \end{block}
    \end{column}
    \begin{column}{0.4\textwidth}
      \begin{figure}
        \includegraphics[width=\textwidth]{20210313_5_tabla_cid.png}
      \end{figure}
    \end{column}

  \end{columns}
  
\end{frame}

\begin{frame}{Comparando kNN-TSPI con SVM y MLP}

  \begin{block}{Hipótesis a contrastar}
    ¿Existen diferencias significativas entre kNN-TSPI y los métodos de Machine Learning más utilizados?
  \end{block}

  \begin{block}{Diseño experimental}
    \begin{itemize}
    \item \textbf{Modelos comparados}: Multilayer Perceptron (ML) y Support Vector Machine (SVM)
    \item \textbf{Tunning} 10-fold cross validation
    \item \textbf{Métricas de validación}: MAPE, POCID y Theil U.
    \end{itemize}
  \end{block}
  
\end{frame}

\begin{frame}{Comparando kNN-TSPI con SVM y MLP}

  \begin{block}{Hipótesis a contrastar}
    ¿Existen diferencias significativas entre kNN-TSPI y los métodos de Machine Learning más utilizados?
  \end{block}

  \begin{block}{Resultados}
    \begin{columns}
      \begin{column}{0.6\textwidth}
        \begin{itemize}
        \item kNN-TSPI presenta resultados intermedios
        \item No presenta diferencias significativas con SVM, un modelo más difícil de ajustar.
        \item Mismos resultados usando MAPE y POCID. Con Theil U, kNN-TSPI muestra mejores resultados.
        \end{itemize}
      \end{column}
      \begin{column}{0.4\textwidth}
        \begin{figure}
          \includegraphics[width=\textwidth]{20210314_1_nemnyi_distancias_tecnicas_cd.png}
        \end{figure}
      \end{column}
      \end{columns}
  \end{block}
\end{frame}




\section{Conclusiones}

\begin{frame}{Discusión}
  \begin{block}{Resumen del trabajo realizado}

    \begin{itemize}
    \item Se presenta un nuevo método de predicción de series temporales basado en similaridad y que corrige los problemas del desfase/amplitud; complejidad invariantes y tratammiento de los aciertos triviales.
    \item Se analizan 6 aproximaciones para tratar el problema de la complejidad invariante, identificando como buena opción la diferencia cuadrática.
    \item Se compara el método con SVM y MLP. No muestra diferencias significativas con SVM y mejora MLP.
    \end{itemize}
  \end{block}

  \begin{block}{Trabajo futuro}
    \begin{itemize}
    \item Continuar analizando las propiedades de kNN-TSPI.
    \item Comparar el algoritmo contra los métodos más utilizados, como el SARIMA.
    \end{itemize}

  \end{block}

\end{frame}


\begin{frame}{}
  \centering \Large
  \emph{Muchas gracias, ¿preguntas?}
\end{frame}

\end{document}